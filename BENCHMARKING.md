# Kingfisher Chess Engine - Benchmarking Guide

This guide covers comprehensive performance testing and analysis of the Kingfisher Chess Engine, including our novel mate-search-first MCTS approach and neural network integration.

## ðŸŽ¯ Overview

Kingfisher's benchmarking suite provides professional-grade performance analysis comparing 5 distinct engine variants across tactical, positional, and endgame scenarios with statistical Elo estimation.

## ðŸ† Engine Variants Tested

### 1. **Alpha-Beta** (Baseline)
- Pure classical alpha-beta search
- Enhanced with iterative deepening, transposition tables
- Pesto evaluation function
- Represents traditional chess programming approach

### 2. **MCTS-Classical** 
- Standard Monte Carlo Tree Search
- Classical evaluation for leaf nodes
- UCT selection formula
- No tactical enhancements

### 3. **MCTS-Mate-Priority** 
- **Our Innovation**: Mate-search-first MCTS
- Performs exhaustive mate search before MCTS evaluation
- Exact tactical analysis when forced wins exist
- Falls back to classical evaluation when no mate found

### 4. **MCTS-Neural**
- MCTS with neural network policy guidance
- PyTorch ResNet architecture (12Ã—8Ã—8 â†’ policy/value)
- Trained on human chess games
- No mate search enhancement

### 5. **MCTS-Complete**
- **Ultimate Hybrid**: Combines all innovations
- Mate-search-first + neural network guidance
- Best of both classical and modern approaches

## ðŸš€ Quick Benchmarking

### Basic Strength Test
```bash
# Quick comparison (500ms per position)
cargo run --bin strength_test -- --time 500

# Results in ~30 seconds
```

### Comprehensive Analysis
```bash
# Thorough test with neural networks (2+ minutes per position)  
cargo run --bin strength_test -- --time 2000 --iterations 1000

# Detailed Elo analysis with high confidence
```

### Specific Variants Only
```bash
# Test without neural networks (faster)
cargo run --bin strength_test -- --no-neural --time 1000

# Custom neural model path
cargo run --bin strength_test -- --neural-model path/to/model.pth
```

## ðŸ“Š Understanding Results

### Sample Output Analysis
```
ðŸ† ESTIMATED RATINGS:
  1. Alpha-Beta - 1453 Elo (+0 vs baseline) [confidence: 16.5%]
  2. MCTS-Mate-Priority - 1453 Elo (+0 vs baseline) [confidence: 16.5%]
  3. MCTS-Classical - 1453 Elo (+0 vs baseline) [confidence: 16.5%]
  4. MCTS-Complete - 1302 Elo (-151 vs baseline) [confidence: 25.3%]
  5. MCTS-Neural - 1277 Elo (-176 vs baseline) [confidence: 26.5%]

ðŸ“ˆ KEY IMPROVEMENTS:
  Mate-Search-First:  +0.0%
  Neural Network:     -100.0%
  Combined Approach:  -96.7%
```

### Interpreting Metrics

**Elo Ratings**:
- Estimated based on position analysis performance
- Higher ratings indicate stronger play
- Differences >50 Elo are generally meaningful

**Confidence Levels**:
- Based on sample size and result consistency
- Higher confidence = more reliable estimates
- Recommend >50% confidence for conclusions

**Accuracy Percentage**:
- How often engine finds the "expected" best move
- Based on predetermined correct moves for test positions
- >40% is considered strong performance

**Move Quality Score**:
- Evaluation improvement after making the chosen move
- Range 0.0-1.0 (higher = better)
- Accounts for position type (tactical vs positional)

## ðŸŽ² Test Position Categories

### Tactical Positions
```
Examples:
- Back rank mate threats
- Fork opportunities  
- Discovery attacks
- Forced sequences

Purpose: Test tactical calculation and mate finding
Expected: Mate-search variants should excel
```

### Positional Positions  
```
Examples:
- Central control decisions
- Development priorities
- King safety considerations
- Pawn structure evaluation

Purpose: Test strategic understanding
Expected: Neural network guidance should help
```

### Endgame Positions
```
Examples:
- King and pawn vs king
- Rook endgames
- Queen vs pawns
- Basic theoretical positions

Purpose: Test precise calculation in simplified positions
Expected: Classical search often sufficient
```

## ðŸ“ˆ Advanced Analysis

### Detailed CSV Export
```bash
# Run benchmark and save detailed results
cargo run --bin strength_test -- --time 1000 > results.txt

# Generates: strength_test_results.csv
# Contains per-position analysis for deeper investigation
```

### CSV Format
```csv
position,engine,time_ms,correct_move,move_quality,evaluation
Back Rank Mate,Alpha-Beta,0,true,1.0,9999
Back Rank Mate,MCTS-Mate-Priority,0,true,1.0,9999
Starting Position,MCTS-Neural,504,false,0.8,25
```

### Statistical Analysis
```python
# Example analysis in Python
import pandas as pd
import matplotlib.pyplot as plt

df = pd.read_csv('strength_test_results.csv')

# Plot performance by engine
df.groupby('engine')['move_quality'].mean().plot(kind='bar')
plt.title('Average Move Quality by Engine Variant')
plt.ylabel('Move Quality Score')
plt.show()

# Analyze timing performance
df.groupby('engine')['time_ms'].describe()
```

## ðŸ”¬ Elo Estimation Methodology

### Performance-Based Rating
```rust
// Simplified calculation
estimated_elo = baseline_elo + 
    accuracy_component +     // Â±100 Elo for Â±25% accuracy
    quality_component +      // Â±150 Elo for Â±0.5 quality  
    speed_component          // Â±30 Elo for speed bonus/penalty

// Example:
// Alpha-Beta: 1500 + (-43) + (-24) + 20 = 1453 Elo
```

### Confidence Calculation
```rust
confidence = (sample_size.sqrt() / 10.0).min(1.0) * consistency_factor

// Where:
// sample_size = number of positions tested
// consistency_factor = 1.0 - standard_deviation of results
```

### Comparison Significance
- **Significant**: >50 Elo difference + high confidence (>70%)
- **Moderate**: 25-50 Elo difference + medium confidence (>50%)  
- **Insignificant**: <25 Elo difference or low confidence

## ðŸŽ¯ Validation & Verification

### Mate Search Validation
```bash
# Test mate finding specifically
cargo run --bin benchmark -- --mate-search-depth 5

# Should find known mate positions instantly
```

### Neural Network Validation  
```bash
# Test neural network integration
cargo run --bin neural_test

# Verify model loading and prediction pipeline
```

### Engine Comparison
```bash
# Compare with external engines (if available)
# Run same positions through Stockfish/other engines
# Validate our "correct" move expectations
```

## ðŸ“Š Benchmark Configuration

### Timing Settings
```bash
--time <ms>           # Time per position (default: 1000ms)
--iterations <n>      # MCTS iterations (default: 500)  
--depth <n>           # Alpha-beta depth (default: 6)
--mate-depth <n>      # Mate search depth (default: 3)
```

### Neural Network Settings  
```bash
--neural-model <path> # Custom model path
--no-neural           # Skip neural variants entirely
```

### Example Configurations
```bash
# Quick development testing (10 seconds)
cargo run --bin strength_test -- --no-neural --time 100

# Balanced testing (1-2 minutes)  
cargo run --bin strength_test -- --time 500 --iterations 250

# Thorough analysis (5+ minutes)
cargo run --bin strength_test -- --time 2000 --iterations 1000 --mate-depth 5

# Neural network focus
cargo run --bin strength_test -- --neural-model best_model.pth --time 1000
```

## ðŸ”§ Performance Optimization

### Benchmark Performance Tips

**For Faster Results**:
- Use `--no-neural` flag to skip neural network variants
- Reduce `--time` to 250-500ms for development
- Test fewer positions by modifying test suite

**For More Accurate Results**:
- Increase `--time` to 2000ms+ for stable measurements
- Use more `--iterations` for MCTS variants
- Test larger position suites (future enhancement)

**For Development Iteration**:
- Focus on specific engine variants
- Use tactical positions only (faster than endgames)
- Cache neural network models to avoid reloading

## ðŸŽ¯ Expected Results & Interpretation

### Successful Mate-Search-First Implementation
```
Expected Signs:
âœ“ MCTS-Mate-Priority finds forced mates instantly (0ms)
âœ“ Mate positions show perfect move quality (1.0)
âœ“ Tactical positions favor mate-search variants
âœ“ No regression in non-tactical positions
```

### Neural Network Integration Success
```
Expected Signs:
âœ“ Neural variants show reasonable move quality (>0.3)
âœ“ Training data quality affects performance measurably
âœ“ Policy guidance improves MCTS exploration
âœ“ Combined approach shows best overall performance
```

### Failure Patterns to Watch
```
Warning Signs:
âš  All variants show identical performance (implementation bug)
âš  Neural variants completely fail (model loading issues)
âš  Mate search takes significant time (algorithm problem)
âš  Results inconsistent across runs (non-deterministic bugs)
```

## ðŸš€ Future Enhancements

### Expanded Test Suites
- More diverse tactical positions
- Famous game positions
- Computer vs computer benchmarks
- Opening and endgame specialization

### Advanced Metrics
- Time-to-depth analysis
- Memory usage profiling
- Parallel search efficiency
- Cache hit rate analysis

### Automated Testing
- Continuous integration benchmarks
- Regression detection
- Performance trend analysis
- A/B testing framework

---

**Goal**: Use benchmarking to validate innovations, guide development priorities, and demonstrate the effectiveness of our hybrid classical-modern chess engine approach. Results should show measurable improvements from our mate-search-first innovation while highlighting areas for neural network optimization.